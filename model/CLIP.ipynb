{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AIC-2024\\AIC-2024\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AIC-2024\\AIC-2024\\venv\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AIC-2024\\AIC-2024\\venv\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AIC-2024\\AIC-2024\\venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AIC-2024\\AIC-2024\\venv\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "#Faster R-CNN\n",
    "faster_rcnn_url = \"https://www.kaggle.com/models/google/faster-rcnn-inception-resnet-v2/tensorFlow1/faster-rcnn-openimages-v4-inception-resnet-v2/1\"\n",
    "model_frcnn = hub.load(faster_rcnn_url).signatures['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((640, 640))  # Resize to 640x640\n",
    "    image_np = np.array(image)\n",
    "    image_np = tf.convert_to_tensor(image_np, dtype=tf.float32)\n",
    "    image_np = tf.image.per_image_standardization(image_np)  # Standardize to zero mean and unit variance\n",
    "    image_np = tf.expand_dims(image_np, 0)  # Add batch dimension\n",
    "    return image_np\n",
    "\n",
    "def preprocess_image_clip(image):\n",
    "    return preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "def object_detection(image_tensor):\n",
    "    results = model_frcnn(image_tensor)\n",
    "    \n",
    "    boxes = results['detection_boxes'].numpy()\n",
    "    scores = results['detection_scores'].numpy()\n",
    "    classes = results['detection_class_entities'].numpy()\n",
    "    \n",
    "    print(\"Detection boxes shape:\", boxes.shape)\n",
    "    print(\"Detection scores shape:\", scores.shape)\n",
    "    print(\"Detection classes shape:\", classes.shape)\n",
    "    \n",
    "    threshold = 0.3\n",
    "    valid_indices = np.where(scores > threshold)[0]\n",
    "    \n",
    "    boxes = boxes[valid_indices]\n",
    "    scores = scores[valid_indices]\n",
    "    classes = classes[valid_indices]\n",
    "    \n",
    "    return boxes, scores, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ 'Con chim',   \n",
    "         'camera',\n",
    "         'người',\n",
    "         'mắt kính',\n",
    "         'chó',\n",
    "         'mèo',\n",
    "         'smartphone']\n",
    "\n",
    "text_inputs = clip.tokenize(texts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_objects(image,boxes):\n",
    "    results = []\n",
    "    for box in boxes:\n",
    "        y_min, x_min, y_max, x_max = box\n",
    "        x_min *= image.width\n",
    "        x_max *= image.width\n",
    "        y_min *= image.height\n",
    "        y_max *= image.height\n",
    "        \n",
    "        x_min = max(0, x_min)\n",
    "        x_max = min(image.width, x_max)\n",
    "        y_min = max(0, y_min)\n",
    "        y_max = min(image.height, y_max)\n",
    "        \n",
    "        object_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "        object_image_input = preprocess_image_clip(object_image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model_clip.encode_image(object_image_input)\n",
    "            text_features = model_clip.encode_text(text_inputs)\n",
    "            \n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        print(\"Similarity scores:\", similarity)\n",
    "        \n",
    "        values, indices = similarity.topk(1)\n",
    "        class_name = texts[indices.item()]\n",
    "        class_scores = similarity[0].cpu().numpy()\n",
    "        results.append((box, class_name, class_scores))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection boxes shape: (100, 4)\n",
      "Detection scores shape: (100,)\n",
      "Detection classes shape: (100,)\n",
      "Similarity scores: tensor([[0.0014, 0.0961, 0.0354, 0.3118, 0.3236, 0.2214, 0.0104]])\n",
      "Similarity scores: tensor([[0.0012, 0.1413, 0.0143, 0.1771, 0.2719, 0.3804, 0.0137]])\n",
      "Similarity scores: tensor([[0.0035, 0.4186, 0.0073, 0.0543, 0.2705, 0.2369, 0.0089]])\n",
      "Similarity scores: tensor([[0.0009, 0.0593, 0.0671, 0.4698, 0.2700, 0.1244, 0.0085]])\n",
      "Similarity scores: tensor([[0.0030, 0.1907, 0.0438, 0.1676, 0.3816, 0.2020, 0.0115]])\n",
      "Similarity scores: tensor([[0.0165, 0.3531, 0.0081, 0.0264, 0.3670, 0.1091, 0.1197]])\n",
      "Detected object: chó\n",
      " - Text: Con chim, Score: 0.00\n",
      " - Text: camera, Score: 0.10\n",
      " - Text: người, Score: 0.04\n",
      " - Text: mắt kính, Score: 0.31\n",
      " - Text: chó, Score: 0.32\n",
      " - Text: mèo, Score: 0.22\n",
      " - Text: smartphone, Score: 0.01\n",
      "Detected object: mèo\n",
      " - Text: Con chim, Score: 0.00\n",
      " - Text: camera, Score: 0.14\n",
      " - Text: người, Score: 0.01\n",
      " - Text: mắt kính, Score: 0.18\n",
      " - Text: chó, Score: 0.27\n",
      " - Text: mèo, Score: 0.38\n",
      " - Text: smartphone, Score: 0.01\n",
      "Detected object: camera\n",
      " - Text: Con chim, Score: 0.00\n",
      " - Text: camera, Score: 0.42\n",
      " - Text: người, Score: 0.01\n",
      " - Text: mắt kính, Score: 0.05\n",
      " - Text: chó, Score: 0.27\n",
      " - Text: mèo, Score: 0.24\n",
      " - Text: smartphone, Score: 0.01\n",
      "Detected object: mắt kính\n",
      " - Text: Con chim, Score: 0.00\n",
      " - Text: camera, Score: 0.06\n",
      " - Text: người, Score: 0.07\n",
      " - Text: mắt kính, Score: 0.47\n",
      " - Text: chó, Score: 0.27\n",
      " - Text: mèo, Score: 0.12\n",
      " - Text: smartphone, Score: 0.01\n",
      "Detected object: chó\n",
      " - Text: Con chim, Score: 0.00\n",
      " - Text: camera, Score: 0.19\n",
      " - Text: người, Score: 0.04\n",
      " - Text: mắt kính, Score: 0.17\n",
      " - Text: chó, Score: 0.38\n",
      " - Text: mèo, Score: 0.20\n",
      " - Text: smartphone, Score: 0.01\n",
      "Detected object: chó\n",
      " - Text: Con chim, Score: 0.02\n",
      " - Text: camera, Score: 0.35\n",
      " - Text: người, Score: 0.01\n",
      " - Text: mắt kính, Score: 0.03\n",
      " - Text: chó, Score: 0.37\n",
      " - Text: mèo, Score: 0.11\n",
      " - Text: smartphone, Score: 0.12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(texts, class_scores):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m draw(image_path, [box \u001b[38;5;28;01mfor\u001b[39;00m box, _ \u001b[38;5;129;01min\u001b[39;00m labels], [label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m labels])\n",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(texts, class_scores):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m draw(image_path, [box \u001b[38;5;28;01mfor\u001b[39;00m box, _ \u001b[38;5;129;01min\u001b[39;00m labels], [label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m labels])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def draw(image_path, boxes, labels):\n",
    "    image = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        y_min, x_min, y_max, x_max = box\n",
    "        x_min *= image.width\n",
    "        x_max *= image.width\n",
    "        y_min *= image.height\n",
    "        y_max *= image.height\n",
    "    \n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min, label, color='r', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "        \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "image_path = '../webapp/static/upload/au.jpg'\n",
    "image_np = preprocess_image(image_path)\n",
    "image = Image.open(image_path)\n",
    "\n",
    "boxes, scores, classes = object_detection(image_np)\n",
    "if len(boxes) == 0:\n",
    "    print(\"No objects detected.\")\n",
    "else:\n",
    "    labels = classify_objects(image, boxes)\n",
    "    for box, label, class_scores in labels:\n",
    "        print(f\"Detected object: {label}\")\n",
    "        for text, score in zip(texts, class_scores):\n",
    "            print(f\" - Text: {text}, Score: {score:.2f}\")\n",
    "    draw(image_path, [box for box, _ in labels], [label for _, label in labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
